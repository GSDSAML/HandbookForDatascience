{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26833a70",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933279f",
   "metadata": {},
   "source": [
    "The dataset originates from the UCI Machine Learning Repository, was collected from homes in suburbs of Boston, Massachusetts in 1978. The data consists of 506 entries, wherein each row has 14 features.\n",
    "\n",
    "This data is quite straightforward. Therefore, we teak it a little bit to increase the complexity of the problem. First, we use SMOTE to increase the size of the original dataset. Second, we randomly remove some cells in rows. Third, we also add noise to the dataset to generate some outlier samples.\n",
    "\n",
    "***References:***\n",
    "- https://www.kaggle.com/datasets/schirmerchad/bostonhoustingmlnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b405c32",
   "metadata": {},
   "source": [
    "|Column   | Description                                                           |\n",
    "|---------|-----------------------------------------------------------------------|\n",
    "| CRIM    | Per capita crime rate by town                                         |\n",
    "| ZN      | Proportion of residential land zoned for lots over 25,000 sq.ft.      |\n",
    "| INDUS   | Proportion of non-retail business acres per town                      |\n",
    "| CHAS    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
    "| NOX     | Nitric oxides concentration (parts per 10 million)                    |\n",
    "| RM      | Average number of rooms per dwelling                                  |\n",
    "| AGE     | Proportion of owner-occupied units built prior to 1940                |\n",
    "| DIS     | Weighted distances to five Boston employment centres                  |\n",
    "| RAD     | Index of accessibility to radial highways                             |\n",
    "| TAX     | Full-value property-tax rate per $10,000                              |\n",
    "| PTRATIO | Pupil - teacher ratio by town                                         |\n",
    "| B       | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town        |\n",
    "| LSTAT   | % lower status of the population                                      |\n",
    "| MEDV    | Median value of owner-occupied homes in $1000's                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8accd7",
   "metadata": {},
   "source": [
    "## 1. Dataset Preprocessing\n",
    "We use a synthesized version of the original dataset, which is 10X bigger and has outlier and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083a72b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/housing_10X_outlier_missing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eaa4b0186b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Skewness of each attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/housing_10X_outlier_missing.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrawdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrawdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/housing_10X_outlier_missing.csv'"
     ]
    }
   ],
   "source": [
    "# Skewness of each attribute\n",
    "# cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
    "rawdata = pd.read_csv('data/housing_10X_outlier_missing.csv', header=0)\n",
    "rawdata[rawdata == -1] = np.nan\n",
    "rawdata.skew().abs().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977f115",
   "metadata": {},
   "source": [
    "### 1.1 Filling missing data & remove outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7ee35",
   "metadata": {},
   "source": [
    "- Replace all missing data (values written as '-1') in the csv file as NaN.\n",
    "- To process outlier, we should consider skewness of each feature. \n",
    "- Outlier detection: \n",
    "    - q3 = np.nanpercentile(data.loc[:,var], 75) #q3 w/o NaN's\n",
    "    - q1 = np.nanpercentile(data.loc[:,var], 25) #q1 w/o NaN's\n",
    "    - threshold = (q3 - q1) * 1.5\n",
    "    - valid_data = [q1 - threshold, threshold + q3]\n",
    "- Outlier values are replaced by either mean or median value\n",
    "    - Skewness <= 1, mean should be used to process outlier\n",
    "    - Skewness > 1, median should be used to process outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3557a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv \n",
    "def load_csv(filename, cols=None, header=None):\n",
    "    data = pd.read_csv(filename, header=header, names=cols)\n",
    "\n",
    "# 0. make all '-1' into NaN (so that -1 does not affect calculations of correlation, mean, median)\n",
    "    data[data == -1] = np.nan\n",
    "\n",
    "# 1. for 'CHAS', just fill in missing data with median\n",
    "    for row in range(data.shape[0]):\n",
    "        if np.isnan(data.loc[row,'CHAS']):\n",
    "            data.loc[row,'CHAS'] = data.loc[:,'CHAS'].median()\n",
    "\n",
    "# 2. for other attributes, calculate skewness of each, and impute median/mean accordingly\n",
    "    for var in data.columns.drop('CHAS'):\n",
    "        abs_skew = abs(data.loc[:,var].skew()) # absolute skewness\n",
    "        med_value = data.loc[:,var].median()  # median without NaN's\n",
    "        mean_value = data.loc[:,var].mean()  # mean without NaN's\n",
    "        \n",
    "        # 2.1 find out outliers, fill with median/mean based on skewness (except CHAS)\n",
    "        # 2.2 impute missing data (NaN) with median/mean based on skewness \n",
    "        q3 = np.nanpercentile(data.loc[:,var], 75) #q3 w/o NaN's\n",
    "        q1 = np.nanpercentile(data.loc[:,var], 25) #q1 w/o NaN's\n",
    "        iqr = q3-q1\n",
    "            \n",
    "        if abs_skew > 1: # for highly skewed attributes -> impute median\n",
    "            for i in range(data.shape[0]):\n",
    "                if data.loc[i,var] > q3 + 1.5*iqr or data.loc[i,var] < q1 - 1.5*iqr:\n",
    "                    data.loc[i,var] = med_value\n",
    "                \n",
    "                # fill in NaN with median\n",
    "                if np.isnan(data.loc[i,var]):\n",
    "                    data.loc[i,var] = med_value\n",
    "\n",
    "        else: # for not too skewed attributes -> impute mean\n",
    "            for i in range(data.shape[0]):\n",
    "                if data.loc[i,var] > q3 + 1.5*iqr or data.loc[i,var] < q1 - 1.5*iqr:\n",
    "                    data.loc[i,var] = mean_value\n",
    "                    \n",
    "                # fill in NaN with mean\n",
    "                if np.isnan(data.loc[i,var]):\n",
    "                    data.loc[i,var] = mean_value\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab39413",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = load_csv('data/housing_10X_outlier_missing.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b790cbc",
   "metadata": {},
   "source": [
    "### 1.2 Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_data.iloc[:,1:14]\n",
    "y = processed_data['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534233ff",
   "metadata": {},
   "source": [
    "## 2. Model Execution\n",
    "\n",
    "Here, we select three algorithms:\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "dt_pred = dtree.predict(X_test)\n",
    "print(\"mae loss of decision tree\", mae(dt_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest regressor\n",
    "regressor_rf = RandomForestRegressor(n_estimators=10, max_depth=5, min_samples_split=10, random_state=0)\n",
    "                       # n_estimators = number of trees\n",
    "rfc = regressor_rf.fit(X_train, y_train)\n",
    "y_pred = regressor_rf.predict(X_test)\n",
    "mae_loss = mae(y_pred, y_test)  # Evaluating the Algorithm by SKLearn\n",
    "print(\"mae loss of random forest:%.2f\" % mae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf90bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRFRegressor().fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "print(\"mae loss of xgb\", mae(xgb_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687c26e",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ff250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = processed_data.iloc[:,1:].corr()\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa8e7d",
   "metadata": {},
   "source": [
    "***Strategy 1: Select high correlated features using Pearson Correlation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abe109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abs(correlation) between attributes and MEDV\n",
    "full_data_corr_wMEDV = corr.iloc[13,:13].abs().sort_values(ascending = False) \n",
    "full_data_corr_wMEDV\n",
    "\n",
    "# get attributes certain high and moderate correlation:\n",
    "high_corr_var = full_data_corr_wMEDV[full_data_corr_wMEDV > 0.5].keys() \n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc9226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model2 = xgb.XGBRFRegressor().fit(X_train[['LSTAT', 'INDUS', 'TAX', 'NOX', 'RM', 'AGE', 'RAD']], y_train)\n",
    "xgb_pred2 = xgb_model2.predict(X_test[['LSTAT', 'INDUS', 'TAX', 'NOX', 'RM', 'AGE', 'RAD']])\n",
    "print(\"mae loss of xgb\", mae(xgb_pred2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72380e6",
   "metadata": {},
   "source": [
    "***Strategy 2: Using Feature Selection Feature of Sklearn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model2 = xgb.XGBRFRegressor()\n",
    "feature_engine = RFE(xgb_model2, n_features_to_select=5)\n",
    "x_train_rfe = feature_engine.fit_transform(X_train, y_train)\n",
    "x_test_rfe = feature_engine.transform(X_test)\n",
    "xgb_model2.fit(x_train_rfe, y_train)\n",
    "rfe_pred = xgb_model2.predict(x_test_rfe)\n",
    "print(\"mae loss of xgb w/ rfe\", mae(rfe_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest = 9999\n",
    "best_n = -1\n",
    "for i in range(13):\n",
    "    temp_model = xgb.XGBRFRegressor()\n",
    "    eng = RFE(temp_model, n_features_to_select=i+1)\n",
    "    x_train_rfe = eng.fit_transform(X_train, y_train)\n",
    "    x_test_rfe = eng.transform(X_test)\n",
    "    temp_model.fit(x_train_rfe, y_train)\n",
    "    rfe_pred = temp_model.predict(x_test_rfe)\n",
    "    score = mae(rfe_pred, y_test)\n",
    "    if score < lowest:\n",
    "        best_n = i + 1\n",
    "        lowest = score\n",
    "print(lowest, best_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest = 9999\n",
    "best_n = -1\n",
    "for i in range(13):\n",
    "    temp_model = DecisionTreeRegressor()\n",
    "    eng = RFE(temp_model, n_features_to_select=i+1)\n",
    "    x_train_rfe = eng.fit_transform(X_train, y_train)\n",
    "    x_test_rfe = eng.transform(X_test)\n",
    "    temp_model.fit(x_train_rfe, y_train)\n",
    "    rfe_pred = temp_model.predict(x_test_rfe)\n",
    "    score = mae(rfe_pred, y_test)\n",
    "    if score < lowest:\n",
    "        best_n = i + 1\n",
    "        lowest = score\n",
    "print(lowest, best_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c9a8",
   "metadata": {},
   "source": [
    "## 4. Model Explanation\n",
    "We will explain model prediction using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdefc3f",
   "metadata": {},
   "source": [
    "### 4.1 SHAP Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d392866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap will automatically select explainer for a given model.\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979801cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "475ee449",
   "metadata": {},
   "source": [
    "Here, we show a global view of feature importance.\n",
    "1. Bee plot helps us understand outcome values corresponding to high/low values of features\n",
    "2. Bar plot presents a global view of feature importance, which don't consider sign (pos/neg) of the contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd048e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8953a57",
   "metadata": {},
   "source": [
    "As can be seen, higher LSTAT values correspond to lower SHAP values. LSTAT is the ratio of lower status of the population. It means that the more low-income population is, the less the price is. Similarly, the bigger the more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015668b",
   "metadata": {},
   "source": [
    "***Then, we show attribution of instance predictions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0].values, features=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[100].values, features=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586584ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.8"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('pyg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "source_map": [
   12,
   16,
   25,
   44,
   50,
   55,
   61,
   65,
   78,
   124,
   126,
   130,
   135,
   139,
   141,
   150,
   157,
   163,
   173,
   177,
   181,
   186,
   191,
   195,
   206,
   210,
   214,
   218,
   228,
   245,
   260,
   265,
   268,
   272,
   278,
   280,
   286,
   288,
   292,
   294,
   298,
   302,
   306
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}