{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Preface The motivation to prepare a handbook for data science started in 2021. Prof. Wen-Syan Li ( profile ) joined the Graduate School of Data Science (GSDS), Seoul National University (SNU) in March 2020, and he taught the Machine Learning & Deep Learning (MLDL) courses in 2020 and 2021 and the Big Data & Knowledge Management System (BKMS) course in 2022. Although there are many textbooks available on the market. However, most books focus on either statistical analysis or theorem aspects of machine learning & deep learning, they do not satisfy one of the essential aspects of data science - hands-on experience and conduct tasks in the data science life cycle end to end. Much information is available on the internet: blob, seminars, conference tutorials, personal websites, Youtube, and online education service providers, like Coursera. It is time-consuming to allocate all the information, especially the information that is evolving and expanding at a fast speed. GSDS at SNU does have a category of the project in its qualifying examination for both master and Ph.D students. However, due to time limitations, the project exam for GSDS features simple problem-solving in the machine learning module training task that lasts for 48 hours. We are not able to evaluate students\u2019 understanding and skills in the full end-to-end data science life cycle, especially data preparation, data augmentation, model performance monitoring, etc, some important tasks a data scientist should be familiar with. In the Fall of 2021, Professor Li proposed to have a project course for data science for both master and Ph.D student-level major students in GSDS, SNU to cover end-to-end tasks in the data science life cycle. The new project course \u201cProject for Data Science\u201d will start in the 2022 Fall semester as a required course for all students in GSDS. The project course aims at training GSDS students to practice their end-to-end skills In preparation for this project course, Professor Li and a few students started to collect content and prepare this Handbook for Learning Data Science. We appreciate the support from the Dean of GSDS, Professor Sang K. Cha for the idea of data science lies in the principle of Algorithms, Big Data, and Computation on the cloud or edge, plus Domain expertise to bring new business models and scientific breakthrough (i.e. domain science). End-to-end practices are essential to data scientists. As a matter of a fact, GSDS, SNU\u2019s required course work ( [GSDS] Curriculum - Graduate School of Data Science (snu.ac.kr) ) since 2022 has aligned to support this direction - two required courses in each column of A, B, and C plus a project course to practice students\u2019 end to end hand-on skill with their own domain expertise. The three pillars of the current revolution in data science are illustrated in Figure 1. Figure 1. The three pillars of the current revolution in data science and the domain We also like to acknowledge the inspiration from a course \u201cFull Stack Deep Learning\u201d offered by UC, Berkeley in 2019 and 2021 https://fullstackdeeplearning.com/ . This course offers not only the theory of deep learning but also substantial materials on all tasks in the end-to-end life cycle (i.e. full stack), including project management, data management, infrastructure, etc. beyond typical school assignments that cover data loading and model training. The Full-Stack Deep Learning course demonstrates the need for a data scientist to master all tasks end to end (i.e. full stack) in the data science life cycle.","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#preface","text":"The motivation to prepare a handbook for data science started in 2021. Prof. Wen-Syan Li ( profile ) joined the Graduate School of Data Science (GSDS), Seoul National University (SNU) in March 2020, and he taught the Machine Learning & Deep Learning (MLDL) courses in 2020 and 2021 and the Big Data & Knowledge Management System (BKMS) course in 2022. Although there are many textbooks available on the market. However, most books focus on either statistical analysis or theorem aspects of machine learning & deep learning, they do not satisfy one of the essential aspects of data science - hands-on experience and conduct tasks in the data science life cycle end to end. Much information is available on the internet: blob, seminars, conference tutorials, personal websites, Youtube, and online education service providers, like Coursera. It is time-consuming to allocate all the information, especially the information that is evolving and expanding at a fast speed. GSDS at SNU does have a category of the project in its qualifying examination for both master and Ph.D students. However, due to time limitations, the project exam for GSDS features simple problem-solving in the machine learning module training task that lasts for 48 hours. We are not able to evaluate students\u2019 understanding and skills in the full end-to-end data science life cycle, especially data preparation, data augmentation, model performance monitoring, etc, some important tasks a data scientist should be familiar with. In the Fall of 2021, Professor Li proposed to have a project course for data science for both master and Ph.D student-level major students in GSDS, SNU to cover end-to-end tasks in the data science life cycle. The new project course \u201cProject for Data Science\u201d will start in the 2022 Fall semester as a required course for all students in GSDS. The project course aims at training GSDS students to practice their end-to-end skills In preparation for this project course, Professor Li and a few students started to collect content and prepare this Handbook for Learning Data Science. We appreciate the support from the Dean of GSDS, Professor Sang K. Cha for the idea of data science lies in the principle of Algorithms, Big Data, and Computation on the cloud or edge, plus Domain expertise to bring new business models and scientific breakthrough (i.e. domain science). End-to-end practices are essential to data scientists. As a matter of a fact, GSDS, SNU\u2019s required course work ( [GSDS] Curriculum - Graduate School of Data Science (snu.ac.kr) ) since 2022 has aligned to support this direction - two required courses in each column of A, B, and C plus a project course to practice students\u2019 end to end hand-on skill with their own domain expertise. The three pillars of the current revolution in data science are illustrated in Figure 1. Figure 1. The three pillars of the current revolution in data science and the domain We also like to acknowledge the inspiration from a course \u201cFull Stack Deep Learning\u201d offered by UC, Berkeley in 2019 and 2021 https://fullstackdeeplearning.com/ . This course offers not only the theory of deep learning but also substantial materials on all tasks in the end-to-end life cycle (i.e. full stack), including project management, data management, infrastructure, etc. beyond typical school assignments that cover data loading and model training. The Full-Stack Deep Learning course demonstrates the need for a data scientist to master all tasks end to end (i.e. full stack) in the data science life cycle.","title":"Preface"},{"location":"SUMMARY/","text":"Welcome Introduction What is Data Science Data Science for Everyone Data Science as a Career Path Content and Target Users of the Handbook Contributors and Feedback Life Cycle Overview Lifecycle of an ML project Business Data Life Cycle MLOps Data Acquisition Data Preparation XAI Privacy, Fairness, Legal Compliance Approaches to Machine Learning Application Development Practical Hands-on skills Step-by-Step Frequent Asked Questions and Answers Check List Self-Evaluation on Projects Progress Infrastructure Overview Data Acquisition, Processing, and Storage Model Training / Evaluation Model Deployment / Monitoring Case Study Sample Excises and Student projects Conclusion","title":"SUMMARY"},{"location":"01.Introduction/","text":"Introduction This part is introduction.","title":"Introduction"},{"location":"01.Introduction/#introduction","text":"This part is introduction.","title":"Introduction"},{"location":"01.Introduction/1/","text":"What is Data Science","title":"What is Data Science"},{"location":"01.Introduction/1/#what-is-data-science","text":"","title":"What is Data Science"},{"location":"01.Introduction/2/","text":"Data Science for Everyone","title":"Data Science for Everyone"},{"location":"01.Introduction/2/#data-science-for-everyone","text":"","title":"Data Science for Everyone"},{"location":"01.Introduction/3/","text":"Data Science as a Career Path","title":"Data Science as a Career Path"},{"location":"01.Introduction/3/#data-science-as-a-career-path","text":"","title":"Data Science as a Career Path"},{"location":"01.Introduction/4/","text":"Content and Target Users of the Handbook","title":"Content and Target Users of the Handbook"},{"location":"01.Introduction/4/#content-and-target-users-of-the-handbook","text":"","title":"Content and Target Users of the Handbook"},{"location":"01.Introduction/5/","text":"Contributors and Feedback","title":"Contributors and Feedback"},{"location":"01.Introduction/5/#contributors-and-feedback","text":"","title":"Contributors and Feedback"},{"location":"02.LifeCycle/","text":"Life Cycle This part is introduction.","title":"Life Cycle"},{"location":"02.LifeCycle/#life-cycle","text":"This part is introduction.","title":"Life Cycle"},{"location":"02.LifeCycle/1/","text":"Overview Essential Capabilities in Data Science Different Roles in Data Science","title":"Overview"},{"location":"02.LifeCycle/1/#overview","text":"","title":"Overview"},{"location":"02.LifeCycle/1/#essential-capabilities-in-data-science","text":"","title":"Essential Capabilities in Data Science"},{"location":"02.LifeCycle/1/#different-roles-in-data-science","text":"","title":"Different Roles in Data Science"},{"location":"02.LifeCycle/2/","text":"Lifecycle of an ML project","title":"Lifecycle of an ML project"},{"location":"02.LifeCycle/2/#lifecycle-of-an-ml-project","text":"","title":"Lifecycle of an ML project"},{"location":"02.LifeCycle/3/","text":"Business Project Goals Prioritizing Projects Other Checklists for Projects","title":"Business"},{"location":"02.LifeCycle/3/#business","text":"","title":"Business"},{"location":"02.LifeCycle/3/#project-goals","text":"","title":"Project Goals"},{"location":"02.LifeCycle/3/#prioritizing-projects","text":"","title":"Prioritizing Projects"},{"location":"02.LifeCycle/3/#other-checklists-for-projects","text":"","title":"Other Checklists for Projects"},{"location":"02.LifeCycle/4/","text":"Data Life Cycle The Validation Set Approach Leave-One-Out Cross-Validation (LOOCV)","title":"Data Life Cycle"},{"location":"02.LifeCycle/4/#data-life-cycle","text":"","title":"Data Life Cycle"},{"location":"02.LifeCycle/4/#the-validation-set-approach","text":"","title":"The Validation Set Approach"},{"location":"02.LifeCycle/4/#leave-one-out-cross-validation-loocv","text":"","title":"Leave-One-Out Cross-Validation (LOOCV)"},{"location":"03.MLOps/","text":"MLOps","title":"MLOps"},{"location":"03.MLOps/#mlops","text":"","title":"MLOps"},{"location":"03.MLOps/1/","text":"Data Acquisition","title":"Data Acquisition"},{"location":"03.MLOps/1/#data-acquisition","text":"","title":"Data Acquisition"},{"location":"03.MLOps/2/","text":"Data Preparation","title":"Data Preparation"},{"location":"03.MLOps/2/#data-preparation","text":"","title":"Data Preparation"},{"location":"04.XAI/","text":"XAI","title":"XAI"},{"location":"04.XAI/#xai","text":"","title":"XAI"},{"location":"05.PFLG/","text":"Privacy, Fairness, Legal Compliance","title":"Privacy, Fairness, Legal Compliance"},{"location":"05.PFLG/#privacy-fairness-legal-compliance","text":"","title":"Privacy, Fairness, Legal Compliance"},{"location":"06.AMLD/","text":"Approaches to Machine Learning Application Development","title":"Approaches to Machine Learning Application Development"},{"location":"06.AMLD/#approaches-to-machine-learning-application-development","text":"","title":"Approaches to Machine Learning Application Development"},{"location":"07.PHOS/","text":"Practical Hands-on skills","title":"Practical Hands-on skills"},{"location":"07.PHOS/#practical-hands-on-skills","text":"","title":"Practical Hands-on skills"},{"location":"07.PHOS/1/","text":"Step-by-Step","title":"Step-by-Step"},{"location":"07.PHOS/1/#step-by-step","text":"","title":"Step-by-Step"},{"location":"07.PHOS/2/","text":"Frequent Asked Questions and Answers","title":"Frequent Asked Questions and Answers"},{"location":"07.PHOS/2/#frequent-asked-questions-and-answers","text":"","title":"Frequent Asked Questions and Answers"},{"location":"07.PHOS/3/","text":"Check List","title":"Check List"},{"location":"07.PHOS/3/#check-list","text":"","title":"Check List"},{"location":"07.PHOS/4/","text":"Check List","title":"Self-Evaluation on Projects Progress"},{"location":"07.PHOS/4/#check-list","text":"","title":"Check List"},{"location":"08.Infrastructure/","text":"Infrastructure","title":"Infrastructure"},{"location":"08.Infrastructure/#infrastructure","text":"","title":"Infrastructure"},{"location":"08.Infrastructure/1/","text":"Overview AI Infrasturcture Infrastructures 1. Scientific computation, Data preparation and wrangling Name Description NumPy NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Scipy A fast, flexible, and expressive Python package for working with \"relational\" or \"labeled\" data. The module is designed to be the building block for doing practical, real-world data analysis in Python. Furthermore, it aims to become the most powerful and flexible open source data analysis and manipulation tool available. Numba Numba is an open source, NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the LLVM compiler project to generate machine code from Python syntax. Numba can compile a large subset of numerically-focused Python, including many NumPy functions. Additionally, Numba has support for automatic parallelization of loops, generation of GPU-accelerated code, and creation of ufuncs and C callbacks. Pandas A fast, flexible, and expressive Python package for working with \"relational\" or \"labeled\" data. The module is designed to be the building block for doing practical, real-world data analysis in Python. Furthermore, it aims to become the most powerful and flexible open source data analysis and manipulation tool available. PySpark PySpark is the collaboration of Apache Spark and Python. In simple terms, it is a Python API for Apache Spark that lets you harness the simplicity of Python as well as the power of Apache Spark to tame Big Data using Python. Polars Polars is a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more. The goal of Polars is to provide a lightning fast DataFrame library that utilizes all available cores on your machine. Dask Dask is a flexible library for parallel computing in Python. Dask is composed of two parts: 1. Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. 2. \"Big Data\" collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers. Vaex Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It calculates statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid for more than a billion (10^9) samples/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted). 2. Specicfic domain data preprocessing 3. Machine learning and deep learning 4. Visualization 5. Development","title":"Overview"},{"location":"08.Infrastructure/1/#overview","text":"AI Infrasturcture","title":"Overview"},{"location":"08.Infrastructure/1/#infrastructures","text":"","title":"Infrastructures"},{"location":"08.Infrastructure/1/#1-scientific-computation-data-preparation-and-wrangling","text":"Name Description NumPy NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. Scipy A fast, flexible, and expressive Python package for working with \"relational\" or \"labeled\" data. The module is designed to be the building block for doing practical, real-world data analysis in Python. Furthermore, it aims to become the most powerful and flexible open source data analysis and manipulation tool available. Numba Numba is an open source, NumPy-aware optimizing compiler for Python sponsored by Anaconda, Inc. It uses the LLVM compiler project to generate machine code from Python syntax. Numba can compile a large subset of numerically-focused Python, including many NumPy functions. Additionally, Numba has support for automatic parallelization of loops, generation of GPU-accelerated code, and creation of ufuncs and C callbacks. Pandas A fast, flexible, and expressive Python package for working with \"relational\" or \"labeled\" data. The module is designed to be the building block for doing practical, real-world data analysis in Python. Furthermore, it aims to become the most powerful and flexible open source data analysis and manipulation tool available. PySpark PySpark is the collaboration of Apache Spark and Python. In simple terms, it is a Python API for Apache Spark that lets you harness the simplicity of Python as well as the power of Apache Spark to tame Big Data using Python. Polars Polars is a lightning fast DataFrame library/in-memory query engine. Its embarrassingly parallel execution, cache efficient algorithms and expressive API makes it perfect for efficient data wrangling, data pipelines, snappy APIs and so much more. The goal of Polars is to provide a lightning fast DataFrame library that utilizes all available cores on your machine. Dask Dask is a flexible library for parallel computing in Python. Dask is composed of two parts: 1. Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads. 2. \"Big Data\" collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers. Vaex Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It calculates statistics such as mean, sum, count, standard deviation etc, on an N-dimensional grid for more than a billion (10^9) samples/rows per second. Visualization is done using histograms, density plots and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).","title":"1. Scientific computation, Data preparation and wrangling"},{"location":"08.Infrastructure/1/#2-specicfic-domain-data-preprocessing","text":"","title":"2. Specicfic domain data preprocessing"},{"location":"08.Infrastructure/1/#3-machine-learning-and-deep-learning","text":"","title":"3. Machine learning and deep learning"},{"location":"08.Infrastructure/1/#4-visualization","text":"","title":"4. Visualization"},{"location":"08.Infrastructure/1/#5-development","text":"","title":"5. Development"},{"location":"08.Infrastructure/2/","text":"Data Acquisition, Processing, and Storage","title":"Data Acquisition, Processing, and Storage"},{"location":"08.Infrastructure/2/#data-acquisition-processing-and-storage","text":"","title":"Data Acquisition, Processing, and Storage"},{"location":"08.Infrastructure/3/","text":"Model Training / Evaluation","title":"Model Training / Evaluation"},{"location":"08.Infrastructure/3/#model-training-evaluation","text":"","title":"Model Training / Evaluation"},{"location":"08.Infrastructure/4/","text":"Model Deployment / Monitoring","title":"Model Deployment / Monitoring"},{"location":"08.Infrastructure/4/#model-deployment-monitoring","text":"","title":"Model Deployment / Monitoring"},{"location":"09.CaseStudy/","text":"Case Study","title":"Case Study"},{"location":"09.CaseStudy/#case-study","text":"","title":"Case Study"},{"location":"10.SESP/","text":"Sample Excises and Student projects","title":"Sample Excises and Student projects"},{"location":"10.SESP/#sample-excises-and-student-projects","text":"","title":"Sample Excises and Student projects"},{"location":"11.Conclusion/","text":"Conclusion","title":"Conclusion"},{"location":"11.Conclusion/#conclusion","text":"","title":"Conclusion"}]}